Got it‚Äîand thanks for the richer brief. I can‚Äôt join your Replit, but here‚Äôs a drop-in ‚ÄúMaster Build Prompt‚Äù you can paste into Replit AI (as its main instruction). It tells the agent exactly what to scaffold for your AI Supervisor‚Äîincluding student scaffolding, teacher corrections, delayed feedback stack, Joy Box, session-based mini-games, flow/microlearning, attention/emotion hints, and gamification.

‚∏ª

üîß Replit AI ‚Äî Master Build Prompt (MetaLingua AI Supervisor)

You are an expert full-stack engineer. Create a self-hosted, low-latency AI Supervisor for MetaLingua called ‚ÄúCallern Supervisor‚Äù. The runtime LLM is Ollama (Llama-3.2) already available on a private server. Build a modular monorepo with production-ready code, tests, and Docker. Prioritize clean architecture, easy swaps, and privacy.

0) Non-functional targets
	‚Ä¢	Latency: live tips under 2‚Äì3s E2E on GPU; hard rate-limit of 1 tip/role/20s to avoid spam.
	‚Ä¢	Privacy: transcripts and video never leave our infra; camera analytics default off and opt-in per user.
	‚Ä¢	Reliability: graceful degradation (if ASR or LLM stalls, UI keeps running; queues buffer).
	‚Ä¢	Internationalization: English + Persian.

1) Monorepo layout

/callern-supervisor
  /web              # Next.js + React + TS (teacher/student dashboards)
  /api              # FastAPI (ASR, LLM, rules, tips, feedback stack, Joy Box, games API)
  /worker           # Celery/RQ worker for aggregation, reports, game builds
  /shared           # Types, prompt templates, rubrics, utils
  /infra            # docker-compose, Dockerfiles, env templates, Nginx/Caddy
  /tests            # Pytests + Playwright E2E
  README.md

2) Core pipeline overview
	1.	Audio ingest (teacher & student separate) ‚Üí WebSocket ‚Üí ASR (Whisper/faster-whisper + Silero VAD).
	2.	Transcript windows (3‚Äì5s, speaker-labeled) ‚Üí LLM event extractor (Ollama Llama-3.2 8B).
	3.	Supervisor engine merges:
	‚Ä¢	Student scaffolding (openers, gap fillers, ideas, collocations, synonyms),
	‚Ä¢	Teacher correction queue (grammar, lexicon, pronunciation notes),
	‚Ä¢	Delayed Feedback Stack (aggregate errors selected by teacher),
	‚Ä¢	Flow/microlearning scheduler (small challenges, spaced refocus).
	4.	Tips emitter (debounced) ‚Üí role-specific panels.
	5.	Report writer (end of session, uses 13B if available) aligned to lesson title/objectives.
	6.	Joy Box (post-session): fetch a very short passage / song snippet / video clip rich in session grammar/lexis.
	7.	Games generator: session ‚Üí new level for vocab/grammar/spelling/pronunciation.
	8.	Attention/affect hints (optional, opt-in): client-side camera heuristics ‚Üí send low-rate events; inform tips/content selection.
	9.	Gamification: teacher quality leaderboard; student progress leaderboard.

3) Frontend (/web)
	‚Ä¢	Next.js + TS, Tailwind, Zustand (state).
	‚Ä¢	Views:
	‚Ä¢	Teacher Dashboard: live transcript, Teacher Tips (subtle, non-interruptive), Error Stack (editable queue), metrics (TTT/STT, turns, wait time), opt-in toggles.
	‚Ä¢	Student Dashboard: Student Tips (scaffolds), targeted word bank, sentence starters, collocations, pronunciation helper, micro-tasks.
	‚Ä¢	After-class: Report, Joy Box widget, Games/Levels.
	‚Ä¢	Leaderboards: teacher quality rank; student progress rank.
	‚Ä¢	WebRTC / Media:
	‚Ä¢	For dev: simple mic capture ‚Üí PCM 16k over WS to /ws/audio?role=teacher|student.
	‚Ä¢	(We will add TURN/SFU later; keep adapter interfaces ready.)
	‚Ä¢	Client-side attention heuristics (opt-in):
	‚Ä¢	Use @mediapipe/face_mesh or tfjs blazeface to compute light features at 1‚Äì2 fps max:
	‚Ä¢	head pose (yaw/pitch), eye aspect ratio (blink), gaze off-screen (rough), mouth aspect ratio (yawn proxy).
	‚Ä¢	Emit hints as probabilistic events (e.g., possible_boredom, possible_confusion) no more than 1/10s and never store raw frames.
	‚Ä¢	UX constraints:
	‚Ä¢	Tips are one-liners with an optional ‚ÄúMore‚Äù expander.
	‚Ä¢	Student ‚Äúscaffold strip‚Äù shows openers, gap fillers, collocations, ideas tied to current topic.
	‚Ä¢	Teacher ‚ÄúDelay button‚Äù pushes an item to the Delayed Feedback Stack for end-of-class review/slide.

4) Backend API (/api) ‚Äî FastAPI

4.1 Realtime endpoints
	‚Ä¢	GET /health
	‚Ä¢	WS /ws/audio?session_id&role ‚Üí receive PCM chunks; ASR partials; return rolling transcripts.
	‚Ä¢	POST /llm/events ‚Üí body: {lesson, objectives, transcript_window} ‚Üí returns strict JSON events.
	‚Ä¢	POST /llm/tips ‚Üí body: {lesson, objectives, events, metrics, context} ‚Üí returns {teacher_tip, student_tip}.
	‚Ä¢	POST /supervisor/stack ‚Üí manage teacher‚Äôs Delayed Feedback Stack (add/remove/list).
	‚Ä¢	GET /metrics/live?session_id ‚Üí stream server-sent events of TTT/STT, turns, wait-time.

4.2 Post-session endpoints
	‚Ä¢	POST /reports/generate ‚Üí produce structured JSON + markdown summary aligned to objectives.
	‚Ä¢	POST /joybox/generate ‚Üí input {lesson, objectives, key_lexis, patterns} ‚Üí output curated very short content item with source link. (Implement Fetcher interface; start with stubs and a small curated corpus; add adapters for YouTube/lyrics later.)
	‚Ä¢	POST /games/generate ‚Üí produce / update level N games (vocab cloze, sentence scramble, match pairs, dictation/pronunciation mimic). Return JSON config playable on /web.

4.3 Admin / Gamification
	‚Ä¢	GET /leaderboards/teachers ‚Üí rank by composite quality score.
	‚Ä¢	GET /leaderboards/students ‚Üí rank by learning velocity (XP gained / time), streaks.
	‚Ä¢	POST /ratings/teacher & POST /ratings/session ‚Üí accept stars + short feedback.

5) Workers (/worker)
	‚Ä¢	Queue framework: Celery (Redis broker) or RQ.
	‚Ä¢	Jobs:
	‚Ä¢	aggregate session events ‚Üí compute hard metrics (TTT/STT, wait-time, error counts),
	‚Ä¢	run Report writer LLM,
	‚Ä¢	generate Joy Box item,
	‚Ä¢	build/update Games level assets,
	‚Ä¢	recalc leaderboards nightly.

6) ASR & diarization
	‚Ä¢	ASR: faster-whisper (CTranslate2), language auto-detect (fa/en), small/medium model.
	‚Ä¢	VAD: Silero; chunk 0.6‚Äì1.2s.
	‚Ä¢	Speaker: rely on role query param (teacher/student). Optional pyannote when tracks are mixed.

7) LLM (Ollama) clients
	‚Ä¢	Use Ollama REST at http://<ollama-host>:11434/api/generate (configurable).
	‚Ä¢	Models:
	‚Ä¢	Live extraction/tips: llama3.2:8b-instruct (fast).
	‚Ä¢	Reports & content: llama3.2:13b-instruct if GPU allows; else 8B.
	‚Ä¢	Always force JSON with robust stop tokens; include temperature=0.2 for determinism.

7.1 Prompt templates (put in /shared/prompts)

Event extractor (system)

You are an ELT classroom event tagger. Output STRICT JSON:
{"events":[
 {"t":"00:07:12","speaker":"student","type":"error_form","target":"present perfect","utt":"I have seen him yesterday"},
 {"t":"00:07:18","speaker":"teacher","type":"feedback_recast","utt":"Oh, you saw him yesterday."}
]}
Allowed types:
ICQ, CCQ, prompt, student_attempt, idea_request, hesitation,
scaffold_request, error_form, error_lex, error_pron,
feedback_recast, feedback_explicit, feedback_elicitation,
praise, instruction, task_transition, check_understanding.
No prose. Reject hallucinated events.

Tip generator (system)

You output at most ONE short actionable tip per role per 20s.
Focus on the NEXT move, not critique.
Teacher tips: pedagogy, staging, ICQ/CCQ, corrective feedback choices.
Student tips: openers, gap fillers, collocations, sentence starters, idea seeds; if hesitation detected, offer 1-2 words only.
Return {"teacher_tip":"...", "student_tip":"..."}; keep each tip ‚â§ 18 words.

Teacher correction validator (system)

If the teacher's utterance is factually or linguistically incorrect, return
{"flag":true,"note":"...","suggestion":"..."} else {"flag":false}.
Keep false positives minimal; be conservative.

Report writer (system)

Write a concise session report aligned to the lesson objectives.
Ground ONLY in provided events + metrics.
Output JSON:
{"session_title":"", "what_student_learned":["..."],
 "evidence":[{"event_ref":2,"note":"..."}],
 "scores":{"accuracy":1-5,"fluency":1-5,"task_completion":1-5},
 "next_steps":["..."]}

Joy Box curator (system)

Given key lexis/grammar, propose ONE ultra-short content item (‚â§120 words text OR ‚â§30s video OR 10-20s song excerpt idea) densely featuring targets.
Return JSON: {"type":"text|video|audio","title":"...","source_hint":"...", "snippet_or_query":"...","why_fit":"..."}
Do NOT invent URLs; if no suitable item, return a precise search query string.

Game generator (system)

Create one micro-game config based on session content. Types:
"cloze", "match", "scramble", "dictation", "pronunciation-mimic".
Return JSON with "type", "instructions", "items":[...], "answer_key".
Keep total playtime 2‚Äì3 minutes (microlearning).

8) Rules/metrics (hybrid: code + LLM)
	‚Ä¢	Compute TTT/STT from timestamps (code, not LLM).
	‚Ä¢	Wait-time: teacher question ‚Üí student first token time.
	‚Ä¢	Hesitation: ASR pauses >1.0s mid-utterance ‚Üí trigger scaffold_request.
	‚Ä¢	Error stack: teacher presses ‚Äústack‚Äù on any event ‚Üí store in queue; UI renders Delayed Feedback section at end with examples/corrections.
	‚Ä¢	Pronunciation: use Whisper tokens + phonemizer for a rough rhyme/mismatch hint; keep as ‚Äúsuggestion,‚Äù not a score.
	‚Ä¢	Flow/microlearning: every ~6‚Äì8 minutes inject a 30‚Äì90s micro-task; show a focus nudge if boredom hint + long wait-times.

9) Database schema (Postgres, /api/db/schema.sql)

TABLE users (
  id uuid pk, role text check (role in ('teacher','student','admin')),
  name text, email text, created_at timestamptz
);

TABLE lessons (
  id uuid pk, course_id uuid, title text,
  objectives jsonb, rubric jsonb, created_at timestamptz
);

TABLE sessions (
  id uuid pk, lesson_id uuid, teacher_id uuid, student_id uuid,
  started_at timestamptz, ended_at timestamptz, metrics jsonb
);

TABLE events (
  id uuid pk, session_id uuid, t_ms int, speaker text,
  type text, payload jsonb
);

TABLE feedback_stack (
  id uuid pk, session_id uuid, event_id uuid, note text, created_at timestamptz
);

TABLE reports (
  id uuid pk, session_id uuid, summary_md text,
  outcomes jsonb, scores jsonb, next_steps jsonb, created_at timestamptz
);

TABLE joybox (
  id uuid pk, session_id uuid, item jsonb, created_at timestamptz
);

TABLE games (
  id uuid pk, session_id uuid, level int, config jsonb, created_at timestamptz
);

TABLE ratings (
  id uuid pk, session_id uuid, teacher_id uuid, student_id uuid,
  stars int, comment text, created_at timestamptz
);

10) Gamification
	‚Ä¢	Teacher Quality Score (per session):
	‚Ä¢	rubric-weighted blend of: appropriate CF usage, ICQ/CCQ presence, TTT/STT target, student improvement delta, student rating.
	‚Ä¢	Student XP:
	‚Ä¢	earned by completing micro-tasks, accuracy streaks, timely responses, game levels.
	‚Ä¢	Nightly job updates leaderboards with decay (recent sessions weigh more).

11) Config & Secrets
	‚Ä¢	.env template: OLAMA_HOST, OLAMA_MODEL_FAST, OLAMA_MODEL_REPORT, DB_URL, REDIS_URL, ASR_MODEL_SIZE, ENABLE_CAMERA_HINTS (default false).
	‚Ä¢	All network targets configurable; write a single settings.py.

12) Docker & Dev
	‚Ä¢	docker-compose.yml with services:
	‚Ä¢	web, api, worker, redis, postgres, (optional) nginx/caddy.
	‚Ä¢	Healthchecks; hot reload in dev; Playwright E2E for tip flow.

13) Tests
	‚Ä¢	Unit tests for: event JSON schema, tip rate-limit, stack operations, metrics.
	‚Ä¢	Integration test with fake transcript to produce: events ‚Üí tips ‚Üí report ‚Üí joybox ‚Üí game config.

14) Initial tasks for the agent
	1.	Scaffold the monorepo structure above.
	2.	Implement /ws/audio with Silero VAD + faster-whisper streaming and return partials.
	3.	Implement OllamaClient with timeout/retries; add prompt templates.
	4.	Implement SupervisorEngine:
	‚Ä¢	merges events ‚Üí metrics,
	‚Ä¢	calls LLM for tips (role-specific),
	‚Ä¢	enforces rate limit (per role).
	5.	Build teacher/student dashboards; wire live tips + stack UI.
	6.	Implement report endpoint + end-of-session job.
	7.	Implement Joy Box (start with curated corpus in /shared/corpus/short_texts.json and return a snippet or a search query string).
	8.	Implement Games generator with 2 game types (cloze, match) and a simple runner in /web/games/[sessionId].
	9.	Add leaderboards and placeholder scoring functions; compute nightly in worker.
	10.	Add opt-in camera heuristics in the client; send low-rate events to /api/attention/hint.

15) Acceptance criteria
	‚Ä¢	Speaking into teacher/student mics produces rolling transcripts.
	‚Ä¢	Every ~5‚Äì10s, each role gets at most one tip; student tips include openers/gap-fillers/collocations/ideas when hesitations occur.
	‚Ä¢	Teacher can push items to Delayed Feedback Stack and see a clean end-of-class review.
	‚Ä¢	End-of-session report JSON + markdown is generated and stored.
	‚Ä¢	Joy Box produces one tiny content item or a precise search query.
	‚Ä¢	A level of games is generated and playable.
	‚Ä¢	Leaderboards render with mock data.
	‚Ä¢	All services run via docker-compose up.

‚∏ª

Notes for the Replit agent
	‚Ä¢	Favor deterministic JSON from LLM (add stop tokens and surrounding guards).
	‚Ä¢	Don‚Äôt block on external media APIs for Joy Box; start with local corpus then leave adapters.
	‚Ä¢	Keep attention/emotion as hints only‚Äînever present as truth.
	‚Ä¢	Include README with setup, env vars, and a 10-minute ‚Äúquick start‚Äù.

‚∏ª

That‚Äôs it. Paste the prompt above into Replit AI and let it scaffold. If you want, I can also give you a smaller MVP prompt (just /ws/audio ‚Üí tips ‚Üí stack, no games/joybox) to test latency first.